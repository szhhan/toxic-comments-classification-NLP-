{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "import emoji\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from nltk import TweetTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_TABLE = str.maketrans(\n",
    "    {\n",
    "        \"\\xad\": None,\n",
    "        \"\\x7f\": None,\n",
    "        \"\\ufeff\": None,\n",
    "        \"\\u200b\": None,\n",
    "        \"\\u200e\": None,\n",
    "        \"\\u202a\": None,\n",
    "        \"\\u202c\": None,\n",
    "        \"‘\": \"'\",\n",
    "        \"’\": \"'\",\n",
    "        \"`\": \"'\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"«\": '\"',\n",
    "        \"»\": '\"',\n",
    "        \"ɢ\": \"G\",\n",
    "        \"ɪ\": \"I\",\n",
    "        \"ɴ\": \"N\",\n",
    "        \"ʀ\": \"R\",\n",
    "        \"ʏ\": \"Y\",\n",
    "        \"ʙ\": \"B\",\n",
    "        \"ʜ\": \"H\",\n",
    "        \"ʟ\": \"L\",\n",
    "        \"ғ\": \"F\",\n",
    "        \"ᴀ\": \"A\",\n",
    "        \"ᴄ\": \"C\",\n",
    "        \"ᴅ\": \"D\",\n",
    "        \"ᴇ\": \"E\",\n",
    "        \"ᴊ\": \"J\",\n",
    "        \"ᴋ\": \"K\",\n",
    "        \"ᴍ\": \"M\",\n",
    "        \"Μ\": \"M\",\n",
    "        \"ᴏ\": \"O\",\n",
    "        \"ᴘ\": \"P\",\n",
    "        \"ᴛ\": \"T\",\n",
    "        \"ᴜ\": \"U\",\n",
    "        \"ᴡ\": \"W\",\n",
    "        \"ᴠ\": \"V\",\n",
    "        \"ĸ\": \"K\",\n",
    "        \"в\": \"B\",\n",
    "        \"м\": \"M\",\n",
    "        \"н\": \"H\",\n",
    "        \"т\": \"T\",\n",
    "        \"ѕ\": \"S\",\n",
    "        \"—\": \"-\",\n",
    "        \"–\": \"-\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_REPLACER = [\n",
    "    (\"sh*t\", \"shit\"),\n",
    "    (\"s**t\", \"shit\"),\n",
    "    (\"f*ck\", \"fuck\"),\n",
    "    (\"fu*k\", \"fuck\"),\n",
    "    (\"f**k\", \"fuck\"),\n",
    "    (\"f*****g\", \"fucking\"),\n",
    "    (\"f***ing\", \"fucking\"),\n",
    "    (\"f**king\", \"fucking\"),\n",
    "    (\"p*ssy\", \"pussy\"),\n",
    "    (\"p***y\", \"pussy\"),\n",
    "    (\"pu**y\", \"pussy\"),\n",
    "    (\"p*ss\", \"piss\"),\n",
    "    (\"b*tch\", \"bitch\"),\n",
    "    (\"bit*h\", \"bitch\"),\n",
    "    (\"h*ll\", \"hell\"),\n",
    "    (\"h**l\", \"hell\"),\n",
    "    (\"cr*p\", \"crap\"),\n",
    "    (\"d*mn\", \"damn\"),\n",
    "    (\"stu*pid\", \"stupid\"),\n",
    "    (\"st*pid\", \"stupid\"),\n",
    "    (\"n*gger\", \"nigger\"),\n",
    "    (\"n***ga\", \"nigger\"),\n",
    "    (\"f*ggot\", \"faggot\"),\n",
    "    (\"scr*w\", \"screw\"),\n",
    "    (\"pr*ck\", \"prick\"),\n",
    "    (\"g*d\", \"god\"),\n",
    "    (\"s*x\", \"sex\"),\n",
    "    (\"a*s\", \"ass\"),\n",
    "    (\"a**hole\", \"asshole\"),\n",
    "    (\"a***ole\", \"asshole\"),\n",
    "    (\"a**\", \"ass\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_REPLACER = []\n",
    "for origin, new in WORDS_REPLACER:\n",
    "    o1 = origin.replace(\"*\", \"\\*\")\n",
    "    REGEX_REPLACER.append((re.compile(o1), new))\n",
    "RE_SPACE = re.compile(r\"\\s\")\n",
    "RE_MULTI_SPACE = re.compile(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_lst = []\n",
    "for i in range(sys.maxunicode+1):\n",
    "    if unicodedata.category(chr(i)) == \"Mn\":\n",
    "        invalid_lst.append(i)\n",
    "NMS_TABLE = dict.fromkeys(ind for ind in invalid_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\n",
    "ARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\n",
    "CHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\n",
    "KANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\n",
    "HIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\n",
    "KATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE = dict()\n",
    "TABLE.update(CUSTOM_TABLE)\n",
    "TABLE.update(NMS_TABLE)\n",
    "TABLE.update(CHINESE_TABLE)\n",
    "TABLE.update(HEBREW_TABLE)\n",
    "TABLE.update(ARABIC_TABLE)\n",
    "TABLE.update(HIRAGANA_TABLE)\n",
    "TABLE.update(KATAKANA_TABLE)\n",
    "TABLE.update(KANJI_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_REGEXP = emoji.get_emoji_regexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNICODE_EMOJI_MY = {}\n",
    "for k, v in emoji.UNICODE_EMOJI_ALIAS.items():\n",
    "    v = v.strip(':')\n",
    "    v = v.replace('_', ' ')\n",
    "    UNICODE_EMOJI_MY[k] = f\" EMJ {v} \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(match):\n",
    "    return UNICODE_EMOJI_MY.get(match.group(0))\n",
    "\n",
    "def my_demojize(string):\n",
    "    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n",
    "\n",
    "def normalize(text):\n",
    "    text = my_demojize(text)\n",
    "    text = RE_SPACE.sub(\" \", text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.translate(TABLE)\n",
    "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
    "    for pattern, repl in REGEX_REPLACER:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORTER_STEMMER = PorterStemmer()\n",
    "LANCASTER_STEMMER = LancasterStemmer()\n",
    "SNOWBALL_STEMMER = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_forms(word):\n",
    "    yield word\n",
    "    yield word.lower()\n",
    "    yield word.upper()\n",
    "    yield word.capitalize()\n",
    "    yield PORTER_STEMMER.stem(word)\n",
    "    yield LANCASTER_STEMMER.stem(word)\n",
    "    yield SNOWBALL_STEMMER.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_get_embedding(word, model):\n",
    "    for form in word_forms(word):\n",
    "        if form in model:\n",
    "            return model[form]\n",
    "\n",
    "    word = word.strip(\"-'\")\n",
    "    for form in word_forms(word):\n",
    "        if form in model:\n",
    "            return model[form]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def gensim_to_embedding_matrix(word2index, path):\n",
    "    model = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((max(word2index.values()) + 1, 300), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "\n",
    "    for word, i in word2index.items():\n",
    "        maybe_embedding = maybe_get_embedding(word, model)\n",
    "        if maybe_embedding is not None:\n",
    "            embedding_matrix[i] = maybe_embedding\n",
    "        else:\n",
    "            unknown_words.append(word)\n",
    "\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "     text_list = pool.map(normalize, data.comment_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = text_list\n",
    "y = np.where(data['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = data[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "yy = np.hstack([y[:, np.newaxis], y_aux_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, yy, test_size=0.3, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "x_train = []\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "\n",
    "for doc in X_train:\n",
    "    word_seq = []\n",
    "    for word in tknzr.tokenize(doc):\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = word_index\n",
    "            word_index += 1\n",
    "        word_seq.append(word_dict[word])\n",
    "    x_train.append(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=200,padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict['unknown-words-in-test'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dd8c1d53474187bee5d20730ebe887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glove_matrix, _ = gensim_to_embedding_matrix(word_dict,\"glove.840B.300d.txt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d026647ba32a476a9aa5c55ac11a7cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_matrix, _ = gensim_to_embedding_matrix(word_dict,\"wiki-news-300d-1M.vec\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476648, 600)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([glove_matrix, wiki_matrix], axis=-1)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for doc in X_test:\n",
    "    word_seq = []\n",
    "    for word in tknzr.tokenize(doc):\n",
    "        if word in word_dict:\n",
    "            word_seq.append(word_dict[word])\n",
    "        else:\n",
    "            word_seq.append(0)\n",
    "    x_test.append(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = sequence.pad_sequences(x_test, maxlen=200,padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 30600,    10, ...,     0,     0,     0],\n",
       "       [   51,    52,   298, ...,     0,     0,     0],\n",
       "       [   95,   187,    10, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 3683,    53,   592, ...,     0,     0,     0],\n",
       "       [  350,     9,    10, ...,     0,     0,     0],\n",
       "       [ 8365,    34,    35, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = len(word_dict)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    \n",
    "        x = x.permute(0, 3, 2, 1) \n",
    "        x = super(SpatialDropout, self).forward(x)  \n",
    "        x = x.permute(0, 3, 2, 1) \n",
    "        x = x.squeeze(2) \n",
    "        return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(features, 600)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(600, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(128 * 2, 128, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(512, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        \n",
    "        self.linear_out = nn.Linear(512, 1)\n",
    "        self.linear_aux_out = nn.Linear(512, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        \n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(embedding_matrix, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "        \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "        \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-16235e9febdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "all_test_preds = []\n",
    "\n",
    "    \n",
    "    \n",
    "test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "all_test_preds.append(test_preds)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-69f1a9bc7abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.cat((torch.tensor([0,0]), torch.tensor([1,1])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
